{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Sportradar's NBA API to build your own database\n",
    "\n",
    "### Introduction\n",
    "\n",
    "My disovery of advanced statistics in sports is one of the things that led me to data science. Watching sports on TV usually means putting up with a slew of tired observations and adages of questionable accuracy. It was fresh and interesting to learn about concepts like value over replacement and points per possession. No one number can capture a player's impact, but advanced statistics are much more useful than the other information available. \n",
    "\n",
    "The NBA is my favorite league and it would be very fun to do data analysis. Before that, I must do the less glamorous task of preparing the data. Which, according to [some estimates](https://www.anaconda.com/state-of-data-science-2020), is 45% of a data scientist's working hours. I identified the [sportradar API](https://developer.sportradar.com/docs/read/basketball/NBA_v7) as an option for data. However, a SQL database would be much more appealing for the data combinations, custom statistics, and other work that would require the data in tabular format. Additionally, we have to consider the cost of requesting data from an API so frequently.\n",
    "\n",
    "Currently, I have been able to create two tables in my NBA database: the schedules table and the seasons table. In this post, I will show how I:\n",
    "\n",
    "1. Connected to the sportradar API\n",
    "2. Downloaded NBA season and schedule data\n",
    "3. Flattened the data and load it into a relational database\n",
    "4. Developed procedures for updates that only download what is necessary\n",
    "\n",
    "Oh, and a final personal note: this is my first Python post! I will be doing more of these moving forward.\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "1. A [sportradar API](https://developer.sportradar.com/docs/read/basketball/NBA_v7) key\n",
    "2. A SQL database that you have successfully connected to and can modify. A tutorial to create and connect to an Azure SQL database in Python can be found [here](https://docs.microsoft.com/en-us/azure/azure-sql/database/connect-query-python?view=azuresql). \n",
    "3. Python and all packages used in the post\n",
    "4. (Optional) Some familiarity with environment variables\n",
    "5. (Optional) An Azure KeyVault account. See how to get set up and interact with Azure KeyVault via Python [here](https://docs.microsoft.com/en-us/azure/key-vault/secrets/quick-create-python?tabs=azure-cli).\n",
    "\n",
    "### Getting set up\n",
    "\n",
    "To begin, let's load the packages and specify the folder paths we will be using. Like previous posts, I will be using a JSON configuration file and environment variables to define many of these things. It keeps information private while still letting me share the fun parts. Additionally, there are a few custom functions from my main development project. Functions from that module connect to the database and my key vault account.\n",
    "\n",
    "The calls to `importlib` and `types` allow me to import the `myFuns` module I have been coding in my main project folder. It is appropriately named: myFuns has all of the functions I have been using to build the database, including connections and secret handling. If you'd prefer not mess with those sorts of things, define the following variables to proceed: \n",
    "\n",
    "1. An object `srNBAKey` that is your sportsradar API key for the NBA v7 API\n",
    "2. An object `cnxn` that is your connection to the SQL database engine\n",
    "3. An object `nbaDir` that will house the downloaded JSON files\n",
    "\n",
    "Now, let's run the code to set up our session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "# Import standard packages\n",
    "import os, importlib.machinery, http.client, json, pandas as pd, time\n",
    "import sqlalchemy, sys\n",
    "import importlib, types # Only needed if loading custom module\n",
    "\n",
    "# Read in Python config file\n",
    "cfFn = os.path.join(os.environ[\"myconfig\"], \"nba.json\")\n",
    "with open(cfFn, \"r\") as f:\n",
    "    cf = json.load(f)\n",
    "\n",
    "# Source custom functions\n",
    "myFunsFn = os.path.join(cf[\"nbaFunsDir\"], \"myFuns/__init__.py\")\n",
    "sys.path.append(cf[\"nbaFunsDir\"])\n",
    "from myFuns.cloud import secman\n",
    "\n",
    "# Get NBA API key from Azure and connect to SQL db\n",
    "seccli = secman.secretClient()\n",
    "srNBAKey = seccli.get_secret(\"srNBAKey\")\n",
    "cnxn = secman.dbEngine(seccli)\n",
    "\n",
    "# Specify directories for output\n",
    "nbaDir = \"D:/nbaBlog\"\n",
    "schedDir = os.path.join(nbaDir, \"schedules\")\n",
    "# os.makedirs(schedDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! We have our API key ready to go and we are connected to our database. Now, let's explore the sportradar API.\n",
    "\n",
    "### Explore the API\n",
    "\n",
    "When you navigate to the [API main page](https://developer.sportradar.com/docs/read/basketball/NBA_v7#nba-api-map), you will be greeted with this image:\n",
    "\n",
    "![sportradar API diagram](https://developer.sportradar.com/files/NBAv7SVG.svg)\n",
    "\n",
    "Quite helpful! Today, we will be working with the Schedule and Seasons endpoints. Let's take a peek at the data. We will start an https connection, send a get request with our API key, and load the JSON payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"5b8b57d1-7290-44f8-b29c-353e865c139e\",\n",
      "    \"year\": 2012,\n",
      "    \"type\": {\n",
      "      \"code\": \"PRE\",\n",
      "      \"name\": \"Pre-season\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"5f45c666-ba68-48d6-a905-f19702ab7e4c\",\n",
      "    \"year\": 2012,\n",
      "    \"type\": {\n",
      "      \"code\": \"PST\",\n",
      "      \"name\": \"Post-season\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Get list of available seasons\n",
    "conn = http.client.HTTPSConnection(\"api.sportradar.us\", timeout = 10)\n",
    "suffix = f\"/nba/trial/v7/en/league/seasons.json?api_key={srNBAKey.value}\"\n",
    "conn.request(\"GET\", suffix)\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "allSeasons = json.loads(data)['seasons']\n",
    "print(json.dumps(allSeasons[0:2], indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schedule has games for the pre-season, post-season, and regular season. Each season has a unique ID and year. Next, let's pull one of these seasons and look at its schedule. We will need the year and type of a season to pull it's schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get year and season type for first listed\n",
    "year = allSeasons[1][\"year\"]\n",
    "seasonType = allSeasons[1][\"type\"][\"code\"]\n",
    "\n",
    "# Pull the data, convert to JSON object\n",
    "# Add some rests to prevent errors\n",
    "time.sleep(1)\n",
    "conn = http.client.HTTPSConnection(\"api.sportradar.us\", timeout = 10)\n",
    "conn.request(\"GET\", f\"/nba/trial/v7/en/games/{year}/{seasonType}/schedule.json?api_key={srNBAKey.value}\")\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "testSched = json.loads(data.decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the database tables\n",
    "\n",
    "Now that we have our two file formats, we can start to think through a database design. Reading through the docs, it seems the table design is not [normalized](https://docs.microsoft.com/en-us/office/troubleshoot/access/database-normalization-description). For example, there is a team profile API endpoint containing most of the team information included in the `home` and `away` entries, so the same information is being presented multiple times. This makes sense: it likely reduces the number of API calls for most users. Still, this underscores the point that the optimal database design is not the same as the optimal API design. We are not working with a particularly sizable dataset, so it should be easy to rebuild the database if we want to change the design later. \n",
    "\n",
    "After some initial experimentation, I decided that these were the variables I wanted:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1570dafb3d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnxn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS scheduleBLOG;\n",
    "CREATE TABLE scheduleBLOG (\n",
    "    schedId varchar(100) PRIMARY KEY        \n",
    "    , srMatchId varchar(100)\n",
    "    , schedDt DATETIME\n",
    "    , homePoints int\n",
    "    , awayPoints int\n",
    "    , coverage varchar(50)\n",
    "    , status varchar(50)    \n",
    "    , trackOnCourt varchar(20)\n",
    "    , IdHome varchar(100)\n",
    "    , srIdHome varchar(100)\n",
    "    , nameHome varchar(100)\n",
    "    , aliasHome varchar(100)\n",
    "    , IdAway varchar(100)\n",
    "    , srIdAway varchar(100)\n",
    "    , nameAway varchar(100)\n",
    "    , aliasAway varchar(100)\n",
    "    , seasonId VARCHAR(100) NOT NULL \n",
    "    , createDt DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    , updatedDt DATETIME     \n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS seasonBLOG;\n",
    "CREATE TABLE seasonBLOG (\n",
    "    seasonId VARCHAR(100) NOT NULL PRIMARY KEY\n",
    "    , seasonType varchar(10) NOT NULL\n",
    "    , seasonYear int NOT NULL\n",
    "    , createDt DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    , updatedDt DATETIME\n",
    ");\n",
    "\n",
    "ALTER TABLE scheduleBLOG\n",
    "ADD CONSTRAINT FK_seasonIdBLOG\n",
    "FOREIGN KEY (seasonId) REFERENCES seasonBLOG(seasonId)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will use the sportradar identifiers as our primary keys in the database for now. We will allow some missing data in the schedule table. After we ingest all of the data, we can think more thoroughly about other constraints to add.\n",
    "\n",
    "The [foreign key](https://www.techopedia.com/definition/7272/foreign-key#:~:text=A%20foreign%20key%20is%20a,establishing%20a%20link%20between%20them.) constraint lets our database know that the schedule and season tables share a common identifier, `seasonId`. That adds some built-in data quality checks. For example, it would not allow you to enter a `seasonId` in the `schedules` table unless that ID were in the `season` table. Formalizing these sorts of relationships help others quickly understand how your database is designed.\n",
    "\n",
    "### Download Data\n",
    "\n",
    "Now that we have explored the data and created our database tables, we will write code to download all data in JSON format and save it in an organized fashion. All files will be named by the convention `schedTYPEYYYY.json` (e.g. `schedPRE2022`). We also want to avoid re-downloading data and making unnecessary API calls. The `os.path.exists` function comes in quite handy. I have already downloaded most of the data, but for the sake of this blog I will delete a few schedules and run the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading schedPRE2012.json\n",
      "downloading schedPRE2013.json\n",
      "downloading schedPRE2014.json\n",
      "downloading schedREG2016.json\n",
      "downloading schedREG2017.json\n"
     ]
    }
   ],
   "source": [
    "# Turn a schedule retrieval into a fnxn\n",
    "def getSchedule(year, seasonType, apiKey):\n",
    "    conn = http.client.HTTPSConnection(\"api.sportradar.us\", timeout = 10)    \n",
    "    conn.request(\"GET\", f\"/nba/trial/v7/en/games/{year}/{seasonType}/schedule.json?api_key={apiKey}\")\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()    \n",
    "    return json.loads(data.decode(\"utf8\"))\n",
    "\n",
    "# Download each season if its key is not yet in the database\n",
    "# Template names based on season type and year\n",
    "for season in allSeasons:         \n",
    "    year = season[\"year\"]\n",
    "    type = season[\"type\"][\"code\"]\n",
    "    fileName = \"sched\" + type + str(year) + \".json\"\n",
    "    outpath = os.path.join(schedDir, fileName)\n",
    "\n",
    "    # Only download file if we have not already    \n",
    "    if not os.path.exists(outpath):\n",
    "        print(\"downloading \"+ fileName)\n",
    "        time.sleep(1)\n",
    "        seas = getSchedule(year = year, seasonType = type, apiKey = srNBAKey.value)\n",
    "        with open(outpath, 'w') as f:               \n",
    "            json.dump(seas, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data\n",
    "\n",
    "Now, we put the data we just downloaded into our database. It will be easiest to rename the JSON data elements to match their SQL-database counterparts. To do that, we use our trustry old friend, the JSON file. The dictionary keys will be the SR API names, and the values our desired values. The JSON file looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schedule\": {\n",
      "    \"id\": \"schedId\",\n",
      "    \"sr_id\": \"srMatchId\",\n",
      "    \"scheduled\": \"schedDt\",\n",
      "    \"home_points\": \"homePoints\",\n",
      "    \"away_points\": \"awayPoints\",\n",
      "    \"coverage\": \"coverage\",\n",
      "    \"status\": \"status\",\n",
      "    \"track_on_court\": \"trackOnCourt\",\n",
      "    \"idHome\": \"idHome\",\n",
      "    \"sr_idHome\": \"srIdHome\",\n",
      "    \"nameHome\": \"nameHome\",\n",
      "    \"aliasHome\": \"aliasHome\",\n",
      "    \"idAway\": \"idAway\",\n",
      "    \"sr_idAway\": \"srIdAway\",\n",
      "    \"nameAway\": \"nameAway\",\n",
      "    \"aliasAway\": \"aliasAway\"\n",
      "  },\n",
      "  \"season\": {\n",
      "    \"id\": \"seasonId\",\n",
      "    \"type\": \"seasonType\",\n",
      "    \"year\": \"seasonYear\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# JSON file with schedionaries for renaming raw to SQL column names\n",
    "with open(\"renameVars.json\", \"r\") as f:\n",
    "    renames = json.load(f)\n",
    "\n",
    "print(json.dumps(renames, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, we should have everything we need to populate our database tables! For both games and seasons, we will want to make sure the record has not been entered before. It also seems there are some seasons with no data available, so the case where the JSON file is empty will need to be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Season schedREG2012.json\n",
      "No games in list schedREG2012.json\n",
      "loading schedREG2012.json\n",
      "skipping schedREG2012.jsondue to no new records\n",
      "Adding Season schedREG2013.json\n",
      "loading schedREG2013.json\n",
      "Adding Season schedREG2014.json\n",
      "loading schedREG2014.json\n",
      "Adding Season schedPRE2015.json\n",
      "loading schedPRE2015.json\n",
      "Adding Season schedREG2015.json\n",
      "loading schedREG2015.json\n",
      "Adding Season schedPRE2016.json\n",
      "loading schedPRE2016.json\n",
      "Adding Season schedPRE2017.json\n",
      "loading schedPRE2017.json\n",
      "Adding Season schedPST2017.json\n",
      "loading schedPST2017.json\n",
      "Adding Season schedPST2018.json\n",
      "loading schedPST2018.json\n",
      "Adding Season schedREG2018.json\n",
      "loading schedREG2018.json\n",
      "Adding Season schedPIT2019.json\n",
      "loading schedPIT2019.json\n",
      "Adding Season schedREG2019.json\n",
      "loading schedREG2019.json\n",
      "Adding Season schedPRE2020.json\n",
      "loading schedPRE2020.json\n",
      "Adding Season schedPIT2020.json\n",
      "loading schedPIT2020.json\n",
      "Adding Season schedREG2020.json\n",
      "loading schedREG2020.json\n",
      "Adding Season schedPRE2021.json\n",
      "loading schedPRE2021.json\n",
      "Adding Season schedPST2021.json\n",
      "loading schedPST2021.json\n",
      "Adding Season schedPIT2021.json\n",
      "loading schedPIT2021.json\n",
      "Adding Season schedREG2021.json\n",
      "loading schedREG2021.json\n"
     ]
    }
   ],
   "source": [
    "games = pd.read_sql(\"SELECT DISTINCT schedId FROM scheduleBLOG\", cnxn)\n",
    "seasons = pd.read_sql(\"SELECT DISTINCT seasonId FROM seasonBLOG\", cnxn)\n",
    "\n",
    "####################################################\n",
    "# MAKE ROWS\n",
    "####################################################\n",
    "# Get all of our schedule JSON files\n",
    "fns = os.listdir(schedDir)\n",
    "\n",
    "# Loop through each schedule year and add to db\n",
    "for fn in fns:     \n",
    "    # Open file, determine year\n",
    "    with open(os.path.join(schedDir, fn)) as f:\n",
    "        sched = json.load(f)\n",
    "    seas = {renames[\"season\"].get(k, k): v for k, v in sched['season'].items()}    \n",
    "    season = pd.DataFrame(seas, index = [0])\n",
    "\n",
    "    # Have we loaded this season before?\n",
    "    newseason = not season['seasonId'].values[0] in seasons['seasonId'].tolist()\n",
    "    if newseason:\n",
    "        print(\"Adding Season \" + fn)\n",
    "        season.to_sql('seasonBLOG', con = cnxn, if_exists = 'append', index = False)\n",
    "\n",
    "    # If no games, skip\n",
    "    if len(sched['games']) == 0:\n",
    "        print(\"No games in list \" + fn)\n",
    "        pass\n",
    "\n",
    "    # Loop through games, extract values into dictionary\n",
    "    out = []    \n",
    "    for game in sched['games']:\n",
    "        newgame = not game['id'] in games['schedId'].tolist()\n",
    "        if newgame:\n",
    "            # Variables of interest from parent values\n",
    "            parentKeep = {'id', 'sr_id', 'status', 'coverage', 'scheduled', \\\n",
    "                'track_on_court', 'home_points', 'away_points'}\n",
    "            parentValues = {x: game[x] for x in parentKeep if x in game}\n",
    "\n",
    "            # We want the same values from \"home\" and \"away\" subdicts\n",
    "            teamKeep = {\"id\", \"sr_id\", \"name\", \"alias\"}\n",
    "            homeValues = {x + 'Home': game['home'][x] for x in teamKeep \\\n",
    "                if x in game['home']}\n",
    "            awayValues = {x + 'Away': game['away'][x] for x in teamKeep \\\n",
    "                if x in game['away']}\n",
    "\n",
    "            # Combine all dicts, rename according to renaming dictionary\n",
    "            # Use None-type if variable is not present\n",
    "            comb = {**parentValues, **homeValues, **awayValues}\n",
    "            res = {renames[\"schedule\"].get(k, k): v for k, v in comb.items()}\n",
    "            notIn = [x for x in renames['schedule'].values() if x not in res]\n",
    "            for n in notIn:\n",
    "                res[n] = None\n",
    "            out.append(res)\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "    # Convert all of our dicts to a DF, then load to DB\n",
    "    print(\"loading \" + fn)\n",
    "    toLoad = pd.DataFrame.from_dict(out)\n",
    "    if not toLoad.empty:\n",
    "        toLoad['schedDt'] = pd.to_datetime(toLoad['schedDt'])\n",
    "        toLoad['seasonId'] = season['seasonId'].values[0]        \n",
    "        toLoad.to_sql('scheduleBLOG', con = cnxn, if_exists = 'append', index = False, \\\n",
    "            dtype = {\"schedDt\": sqlalchemy.DateTime})\n",
    "    else:\n",
    "        print(\"skipping \" + fn + \"due to no new records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that worked. We will query the tables we loaded to and print out a few rows. Did it work? Did it fail? The suspense! The intrigue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our newly loaded data\n",
    "games = pd.read_sql(\"SELECT TOP 10 * FROM scheduleBLOG\", cnxn)\n",
    "seasons = pd.read_sql(\"SELECT TOP 10 * FROM seasonBLOG\", cnxn)\n",
    "\n",
    "# Print a few games out\n",
    "print(games)\n",
    "# And a few seasons\n",
    "print(seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks to have worked. Eventually, a custom primary key that sorts chronologically would be nice. But, we have a good prototype to work with. We can now download some game/season statistic data and start having some real fun in the coming weeks and months. At least it will give me something to do this NBA season while I nervously wait for Chet Holmgren to [heal from his severe foot injury](https://theathletic.com/3535418/2022/08/24/thunder-chet-holmgren-out-for-season-lisfranc-injury/). Feel better soon, Chet!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "52d5c7957a8de22acba2095602af51ba4a5f9cf4afefb365b970c2f72199ae60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
