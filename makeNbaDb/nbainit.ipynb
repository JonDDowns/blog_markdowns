{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Sportradar's NBA API to build your own database\n",
    "\n",
    "### Introduction\n",
    "\n",
    "My disovery of advanced statistics in sports is one of the things that led me to data science. Watching sports on TV usually means putting up with a slew of tired observations and adages of questionable accuracy. It was fresh and interesting to learn about concepts like value over replacement and points per possession. No one number can capture a player's impact, but advanced statistics are much more useful than the other information available. \n",
    "\n",
    "The NBA is my favorite league and it would be very fun to do data analysis. Before that, I must do the less glamorous task of preparing the data. Which, according to [some estimates](https://www.anaconda.com/state-of-data-science-2020), is 45% of a data scientist's working hours. I identified the [sportradar API](https://developer.sportradar.com/docs/read/basketball/NBA_v7) as an option for data. However, a SQL database would be much more appealing for the data combinations, custom statistics, and other work that would require the data in tabular format. Additionally, we have to consider the cost of requesting data from an API so frequently.\n",
    "\n",
    "Currently, I have been able to create two tables in my NBA database: the schedules table and the seasons table. In this post, I will show how I:\n",
    "\n",
    "1. Connected to the sportradar API\n",
    "2. Downloaded NBA season and schedule data\n",
    "3. Flattened the data and load it into a relational database\n",
    "4. Developed procedures for updates that only download what is necessary\n",
    "\n",
    "Oh, and a final personal note: this is my first Python post! I will be doing more of these moving forward.\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "1. A [sportradar API](https://developer.sportradar.com/docs/read/basketball/NBA_v7) key\n",
    "2. A SQL database that you have successfully connected to and can modify. A tutorial to create and connect to an Azure SQL database in Python can be found [here](https://docs.microsoft.com/en-us/azure/azure-sql/database/connect-query-python?view=azuresql). \n",
    "3. Python and all packages used in the post\n",
    "4. (Optional) Some familiarity with environment variables\n",
    "5. (Optional) An Azure KeyVault account. See how to get set up and interact with Azure KeyVault via Python [here](https://docs.microsoft.com/en-us/azure/key-vault/secrets/quick-create-python?tabs=azure-cli).\n",
    "\n",
    "### Getting set up\n",
    "\n",
    "To begin, let's load the packages and specify the folder paths we will be using. Like previous posts, I will be using a JSON configuration file and environment variables to define many of these things. It keeps information private while still letting me share the fun parts. Additionally, there are a few custom functions from my main development project. Functions from that module connect to the database and my key vault account.\n",
    "\n",
    "The calls to `importlib` and `types` allow me to import the `myFuns` module I have been coding in my main project folder. It is appropriately named: myFuns has all of the functions I have been using to build the database, including connections and secret handling. If you'd prefer not mess with those sorts of things, define the following variables to proceed: \n",
    "\n",
    "1. An object `srNBAKey` that is your sportsradar API key for the NBA v7 API\n",
    "2. An object `cnxn` that is your connection to the SQL database engine\n",
    "3. An object `nbaDir` that will house the downloaded JSON files\n",
    "\n",
    "Now, let's run the code to set up our session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "# Import standard packages\n",
    "import os, importlib.machinery, http.client, json, pandas as pd, time\n",
    "import sqlalchemy\n",
    "import importlib, types # Only needed if loading custom module\n",
    "\n",
    "# Read in Python config file\n",
    "cfFn = os.path.join(os.environ[\"myconfig\"], \"nba.json\")\n",
    "with open(cfFn, \"r\") as f:\n",
    "    cf = json.load(f)\n",
    "\n",
    "# Source custom functions\n",
    "myFunsFn = os.path.join(cf[\"nbaFunsDir\"], \"myFuns/__init__.py\")\n",
    "sys.path.append(cf[\"nbaFunsDir\"])\n",
    "from myFuns.cloud import secman\n",
    "\n",
    "# Get NBA API key from Azure and connect to SQL db\n",
    "seccli = secman.secretClient()\n",
    "srNBAKey = seccli.get_secret(\"srNBAKey\")\n",
    "cnxn = secman.dbEngine(seccli)\n",
    "\n",
    "# Specify directories for output\n",
    "nbaDir = \"D:/nbaBlog\"\n",
    "schedDir = os.path.join(nbaDir, \"schedules\")\n",
    "# os.makedirs(schedDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! We have our API key ready to go and we are connected to our database. Now, let's explore the sportradar API.\n",
    "\n",
    "### Explore the API\n",
    "\n",
    "When you navigate to the [API main page](https://developer.sportradar.com/docs/read/basketball/NBA_v7#nba-api-map), you will be greeted with this image:\n",
    "\n",
    "![sportradar API diagram](https://developer.sportradar.com/files/NBAv7SVG.svg)\n",
    "\n",
    "Quite helpful! Today, we will be working with the Schedule and Seasons endpoints. Let's take a peek at the data. We will start an https connection, send a get request with our API key, and load the JSON payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"5b8b57d1-7290-44f8-b29c-353e865c139e\",\n",
      "    \"year\": 2012,\n",
      "    \"type\": {\n",
      "      \"code\": \"PRE\",\n",
      "      \"name\": \"Pre-season\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"5f45c666-ba68-48d6-a905-f19702ab7e4c\",\n",
      "    \"year\": 2012,\n",
      "    \"type\": {\n",
      "      \"code\": \"PST\",\n",
      "      \"name\": \"Post-season\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Get list of available seasons\n",
    "conn = http.client.HTTPSConnection(\"api.sportradar.us\", timeout = 10)\n",
    "suffix = f\"/nba/trial/v7/en/league/seasons.json?api_key={srNBAKey.value}\"\n",
    "conn.request(\"GET\", suffix)\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "allSeasons = json.loads(data)['seasons']\n",
    "print(json.dumps(allSeasons[0:2], indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schedule has games for the pre-season, post-season, and regular season. Each season has a unique ID and year. Next, let's pull one of these seasons and look at its schedule. We will need the year and type of a season to pull it's schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Get year and season type for first listed\n",
    "year = allSeasons[1][\"year\"]\n",
    "seasonType = allSeasons[1][\"type\"][\"code\"]\n",
    "\n",
    "# Pull the data, convert to JSON object\n",
    "# Add some rests to prevent errors\n",
    "conn = http.client.HTTPSConnection(\"api.sportradar.us\", timeout = 10)\n",
    "conn.request(\"GET\", f\"/nba/trial/v7/en/games/{year}/{seasonType}/schedule.json?api_key={srNBAKey.value}\")\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "testSched = json.loads(data.decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the database tables\n",
    "\n",
    "Now that we have our two file formats, we can start to think through a database design. Reading through the docs, it seems the table design is not [normalized](https://docs.microsoft.com/en-us/office/troubleshoot/access/database-normalization-description). For example, there is a team profile API endpoint containing most of the team information included in the `home` and `away` entries, so the same information is being presented multiple times. This makes sense: it likely reduces the number of API calls for most users. Still, this underscores the point that the optimal database design is not the same as the optimal API design. We are not working with a particularly sizable dataset, so it should be easy to rebuild the database if we want to change the design later. \n",
    "\n",
    "After some initial experimentation, I decided that these were the variables I wanted:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x6b5a0e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnxn.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS scheduleBLOG;\n",
    "CREATE TABLE scheduleBLOG (\n",
    "    schedId varchar(100) PRIMARY KEY        \n",
    "    , srMatchId varchar(100)\n",
    "    , schedDt DATETIME\n",
    "    , homePoints int\n",
    "    , awayPoints int\n",
    "    , coverage varchar(50)\n",
    "    , status varchar(50)    \n",
    "    , trackOnCourt varchar(20)\n",
    "    , IdHome varchar(100)\n",
    "    , srIdHome varchar(100)\n",
    "    , nameHome varchar(100)\n",
    "    , aliasHome varchar(100)\n",
    "    , IdAway varchar(100)\n",
    "    , srIdAway varchar(100)\n",
    "    , nameAway varchar(100)\n",
    "    , aliasAway varchar(100)\n",
    "    , seasonId VARCHAR(100) NOT NULL \n",
    "    , createDt DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    , updatedDt DATETIME     \n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS seasonBLOG;\n",
    "CREATE TABLE seasonBLOG (\n",
    "    seasonId VARCHAR(100) NOT NULL PRIMARY KEY\n",
    "    , seasonType varchar(10) NOT NULL\n",
    "    , seasonYear int NOT NULL\n",
    "    , createDt DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    , updatedDt DATETIME\n",
    ");\n",
    "\n",
    "ALTER TABLE scheduleBLOG\n",
    "ADD CONSTRAINT FK_seasonIdBLOG\n",
    "FOREIGN KEY (seasonId) REFERENCES seasonBLOG(seasonId)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will use the sportradar identifiers as our primary keys in the database for now. We will allow some missing data in the schedule table. After we ingest all of the data, we can think more thoroughly about other constraints to add.\n",
    "\n",
    "The [foreign key](https://www.techopedia.com/definition/7272/foreign-key#:~:text=A%20foreign%20key%20is%20a,establishing%20a%20link%20between%20them.) constraint lets our database know that the schedule and season tables share a common identifier, `seasonId`. That adds some built-in data quality checks. For example, it would not allow you to enter a `seasonId` in the `schedules` table unless that ID were in the `season` table. Formalizing these sorts of relationships help others quickly understand how your database is designed.\n",
    "\n",
    "### Download Data\n",
    "\n",
    "Now that we have explored the data and created our database tables, we will write code to download all data in JSON format and save it in an organized fashion. All files will be named by the convention `schedTYPEYYYY.json` (e.g. `schedPRE2022`). We also want to avoid re-downloading data and making unnecessary API calls. The `os.path.exists` function comes in quite handy. I have already downloaded most of the data, but for the sake of this blog I will delete a few schedules and run the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading schedPST2014.json\n",
      "downloading schedPST2015.json\n",
      "downloading schedPST2016.json\n",
      "downloading schedPRE2022.json\n"
     ]
    }
   ],
   "source": [
    "# Turn a schedule retrieval into a fnxn\n",
    "def getSchedule(year, seasonType, apiKey):\n",
    "    conn = http.client.HTTPSConnection(\"api.sportradar.us\", timeout = 10)    \n",
    "    conn.request(\"GET\", f\"/nba/trial/v7/en/games/{year}/{seasonType}/schedule.json?api_key={apiKey}\")\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()    \n",
    "    return json.loads(data.decode(\"utf8\"))\n",
    "\n",
    "# Download each season if its key is not yet in the database\n",
    "# Template names based on season type and year\n",
    "for season in allSeasons:         \n",
    "    year = season[\"year\"]\n",
    "    type = season[\"type\"][\"code\"]\n",
    "    fileName = \"sched\" + type + str(year) + \".json\"\n",
    "    outpath = os.path.join(schedDir, fileName)\n",
    "\n",
    "    # Only download file if we have not already    \n",
    "    if not os.path.exists(outpath):\n",
    "        print(\"downloading \"+ fileName)\n",
    "        time.sleep(1)\n",
    "        seas = getSchedule(year = year, seasonType = type, apiKey = srNBAKey.value)\n",
    "        with open(outpath, 'w') as f:               \n",
    "            json.dump(seas, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data\n",
    "\n",
    "Now, we put the data we just downloaded into our database. It will be easiest to rename the JSON data elements to match their SQL-database counterparts. To do that, we use our trustry old friend, the JSON file. The dictionary keys will be the SR API names, and the values our desired values. The JSON file looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schedule\": {\n",
      "    \"id\": \"schedId\",\n",
      "    \"sr_id\": \"srMatchId\",\n",
      "    \"scheduled\": \"schedDt\",\n",
      "    \"home_points\": \"homePoints\",\n",
      "    \"away_points\": \"awayPoints\",\n",
      "    \"coverage\": \"coverage\",\n",
      "    \"status\": \"status\",\n",
      "    \"track_on_court\": \"trackOnCourt\",\n",
      "    \"idHome\": \"idHome\",\n",
      "    \"sr_idHome\": \"srIdHome\",\n",
      "    \"nameHome\": \"nameHome\",\n",
      "    \"aliasHome\": \"aliasHome\",\n",
      "    \"idAway\": \"idAway\",\n",
      "    \"sr_idAway\": \"srIdAway\",\n",
      "    \"nameAway\": \"nameAway\",\n",
      "    \"aliasAway\": \"aliasAway\"\n",
      "  },\n",
      "  \"season\": {\n",
      "    \"id\": \"seasonId\",\n",
      "    \"type\": \"seasonType\",\n",
      "    \"year\": \"seasonYear\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# JSON file with schedionaries for renaming raw to SQL column names\n",
    "with open(\"renameVars.json\", \"r\") as f:\n",
    "    renames = json.load(f)\n",
    "\n",
    "print(json.dumps(renames, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, we should have everything we need to populate our database tables! For both games and seasons, we will want to make sure the record has not been entered before. It also seems there are some seasons with no data available, so the case where the JSON file is empty will need to be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading schedPRE2012.json\n",
      "skipping schedPRE2012.jsondue to no new records\n",
      "loading schedREG2012.json\n",
      "skipping schedREG2012.jsondue to no new records\n",
      "loading schedREG2013.json\n",
      "Adding Season schedPRE2014.json\n",
      "loading schedPRE2014.json\n",
      "Adding Season schedREG2014.json\n",
      "loading schedREG2014.json\n",
      "Adding Season schedPRE2015.json\n",
      "loading schedPRE2015.json\n",
      "Adding Season schedREG2015.json\n",
      "loading schedREG2015.json\n",
      "Adding Season schedPRE2016.json\n",
      "loading schedPRE2016.json\n",
      "Adding Season schedREG2016.json\n",
      "loading schedREG2016.json\n",
      "Adding Season schedPRE2017.json\n",
      "loading schedPRE2017.json\n",
      "Adding Season schedPST2017.json\n",
      "loading schedPST2017.json\n",
      "Adding Season schedREG2017.json\n",
      "loading schedREG2017.json\n",
      "Adding Season schedPST2018.json\n",
      "loading schedPST2018.json\n",
      "Adding Season schedREG2018.json\n",
      "loading schedREG2018.json\n",
      "Adding Season schedPIT2019.json\n",
      "loading schedPIT2019.json\n",
      "Adding Season schedREG2019.json\n",
      "loading schedREG2019.json\n",
      "Adding Season schedPRE2020.json\n",
      "loading schedPRE2020.json\n",
      "Adding Season schedPIT2020.json\n",
      "loading schedPIT2020.json\n",
      "Adding Season schedREG2020.json\n",
      "loading schedREG2020.json\n",
      "Adding Season schedPRE2021.json\n",
      "loading schedPRE2021.json\n",
      "Adding Season schedPST2021.json\n",
      "loading schedPST2021.json\n",
      "Adding Season schedPIT2021.json\n",
      "loading schedPIT2021.json\n",
      "Adding Season schedREG2021.json\n",
      "loading schedREG2021.json\n",
      "Adding Season schedPST2022.json\n",
      "loading schedPST2022.json\n",
      "skipping schedPST2022.jsondue to no new records\n",
      "Adding Season schedPIT2022.json\n",
      "loading schedPIT2022.json\n",
      "skipping schedPIT2022.jsondue to no new records\n",
      "Adding Season schedREG2022.json\n",
      "loading schedREG2022.json\n",
      "Adding Season schedPST2012.json\n",
      "loading schedPST2012.json\n",
      "Adding Season schedPRE2013.json\n",
      "loading schedPRE2013.json\n",
      "Adding Season schedPRE2018.json\n",
      "loading schedPRE2018.json\n",
      "Adding Season schedPST2019.json\n",
      "loading schedPST2019.json\n",
      "Adding Season schedPST2013.json\n",
      "loading schedPST2013.json\n",
      "Adding Season schedPRE2019.json\n",
      "loading schedPRE2019.json\n",
      "Adding Season schedPST2020.json\n",
      "loading schedPST2020.json\n",
      "Adding Season schedPST2014.json\n",
      "loading schedPST2014.json\n",
      "Adding Season schedPST2015.json\n",
      "loading schedPST2015.json\n",
      "Adding Season schedPST2016.json\n",
      "loading schedPST2016.json\n",
      "Adding Season schedPRE2022.json\n",
      "loading schedPRE2022.json\n"
     ]
    }
   ],
   "source": [
    "games = pd.read_sql(\"SELECT DISTINCT schedId FROM scheduleBLOG\", cnxn)\n",
    "seasons = pd.read_sql(\"SELECT DISTINCT seasonId FROM seasonBLOG\", cnxn)\n",
    "\n",
    "####################################################\n",
    "# MAKE ROWS\n",
    "####################################################\n",
    "# Get all of our schedule JSON files\n",
    "fns = os.listdir(schedDir)\n",
    "\n",
    "# Loop through each schedule year and add to db\n",
    "for fn in fns:     \n",
    "    # Open file, determine year\n",
    "    with open(os.path.join(schedDir, fn)) as f:\n",
    "        sched = json.load(f)\n",
    "    seas = {renames[\"season\"].get(k, k): v for k, v in sched['season'].items()}    \n",
    "    season = pd.DataFrame(seas, index = [0])\n",
    "\n",
    "    # Have we loaded this season before?\n",
    "    newseason = not season['seasonId'].values[0] in seasons['seasonId'].tolist()\n",
    "    if newseason:\n",
    "        print(\"Adding Season \" + fn)\n",
    "        season.to_sql('seasonBLOG', con = cnxn, if_exists = 'append', index = False)\n",
    "\n",
    "    # If no games, skip\n",
    "    if len(sched['games']) == 0:\n",
    "        print(\"No games in list \" + fn)\n",
    "        pass\n",
    "\n",
    "    # Loop through games, extract values into dictionary\n",
    "    out = []    \n",
    "    for game in sched['games']:\n",
    "        newgame = not game['id'] in games['schedId'].tolist()\n",
    "        if newgame:\n",
    "            # Variables of interest from parent values\n",
    "            parentKeep = {'id', 'sr_id', 'status', 'coverage', 'scheduled', \\\n",
    "                'track_on_court', 'home_points', 'away_points'}\n",
    "            parentValues = {x: game[x] for x in parentKeep if x in game}\n",
    "\n",
    "            # We want the same values from \"home\" and \"away\" subdicts\n",
    "            teamKeep = {\"id\", \"sr_id\", \"name\", \"alias\"}\n",
    "            homeValues = {x + 'Home': game['home'][x] for x in teamKeep \\\n",
    "                if x in game['home']}\n",
    "            awayValues = {x + 'Away': game['away'][x] for x in teamKeep \\\n",
    "                if x in game['away']}\n",
    "\n",
    "            # Combine all dicts, rename according to renaming dictionary\n",
    "            # Use None-type if variable is not present\n",
    "            comb = {**parentValues, **homeValues, **awayValues}\n",
    "            res = {renames[\"schedule\"].get(k, k): v for k, v in comb.items()}\n",
    "            notIn = [x for x in renames['schedule'].values() if x not in res]\n",
    "            for n in notIn:\n",
    "                res[n] = None\n",
    "            out.append(res)\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "    # Convert all of our dicts to a DF, then load to DB\n",
    "    print(\"loading \" + fn)\n",
    "    toLoad = pd.DataFrame.from_dict(out)\n",
    "    if not toLoad.empty:\n",
    "        toLoad['schedDt'] = pd.to_datetime(toLoad['schedDt'])\n",
    "        toLoad['seasonId'] = season['seasonId'].values[0]        \n",
    "        toLoad.to_sql('scheduleBLOG', con = cnxn, if_exists = 'append', index = False, \\\n",
    "            dtype = {\"schedDt\": sqlalchemy.DateTime})\n",
    "    else:\n",
    "        print(\"skipping \" + fn + \"due to no new records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that worked. We will query the tables we loaded to and print out a few rows. Did it work? Did it fail? The suspense! The intrigue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                schedId          srMatchId  \\\n",
      "0  000193f7-3433-461a-b562-7f7c69e8023f               None   \n",
      "1  000477ca-9053-4bdf-857b-629ebc8e670e  sr:match:24750712   \n",
      "2  00096a31-9afb-4d89-957f-2ca5741f813c  sr:match:12233334   \n",
      "3  000bee26-0d7f-451e-b2c3-b52f24581c7f   sr:match:7790366   \n",
      "4  000e7d9f-7fa0-4273-a419-e02a95cd6101   sr:match:9956289   \n",
      "5  0010abb3-bf15-401a-97fd-a20d7f45ef3b               None   \n",
      "6  00189aa2-fe36-420e-b3a1-7d9dbd8846a1  sr:match:28808730   \n",
      "7  001b86bd-1891-462f-8d0b-c373434d4f14  sr:match:15327444   \n",
      "8  001da480-f3ad-4da4-a72b-9899f2394672  sr:match:28809934   \n",
      "9  001eaa7b-4a2d-4077-b590-1f288a42a8a2   sr:match:4194739   \n",
      "\n",
      "              schedDt  homePoints  awayPoints coverage     status  \\\n",
      "0 2023-04-08 00:30:00         NaN         NaN     full  scheduled   \n",
      "1 2021-02-27 03:00:00       130.0       121.0     full     closed   \n",
      "2 2017-12-14 00:00:00        95.0       106.0     full     closed   \n",
      "3 2015-10-31 00:00:00       113.0       118.0     full     closed   \n",
      "4 2017-01-21 01:00:00       107.0        91.0     full     closed   \n",
      "5 2023-02-04 02:00:00         NaN         NaN     full  scheduled   \n",
      "6 2022-01-03 00:00:00        99.0       133.0     full     closed   \n",
      "7 2018-11-01 00:00:00       128.0       125.0     full     closed   \n",
      "8 2022-02-17 00:30:00       113.0       108.0     full     closed   \n",
      "9 2014-02-09 20:30:00        86.0        92.0     full     closed   \n",
      "\n",
      "  trackOnCourt                                IdHome      srIdHome  \\\n",
      "0            1  583ecf50-fb46-11e1-82cb-f4ce4684ea4c  sr:team:3411   \n",
      "1            1  583ec825-fb46-11e1-82cb-f4ce4684ea4c  sr:team:3428   \n",
      "2            1  583ed157-fb46-11e1-82cb-f4ce4684ea4c  sr:team:3437   \n",
      "3         None  583ecefd-fb46-11e1-82cb-f4ce4684ea4c  sr:team:3410   \n",
      "4         None  583eca88-fb46-11e1-82cb-f4ce4684ea4c  sr:team:3415   \n",
      "5            1  583ece50-fb46-11e1-82cb-f4ce4684ea4c  sr:team:3434   \n",
      "6            1  583ec97e-fb46-11e1-82cb-f4ce4684ea4c  sr:team:3430   \n",
      "7            1  583eca2f-fb46-11e1-82cb-f4ce4684ea4c  sr:team:3426   \n",
      "8            1  583ec7cd-fb46-11e1-82cb-f4ce4684ea4c  sr:team:3419   \n",
      "9         None  583ecae2-fb46-11e1-82cb-f4ce4684ea4c  sr:team:3427   \n",
      "\n",
      "                 nameHome aliasHome                                IdAway  \\\n",
      "0        Dallas Mavericks       DAL  583ec5fd-fb46-11e1-82cb-f4ce4684ea4c   \n",
      "1   Golden State Warriors       GSW  583ec97e-fb46-11e1-82cb-f4ce4684ea4c   \n",
      "2           Orlando Magic       ORL  583ecdfb-fb46-11e1-82cb-f4ce4684ea4c   \n",
      "3         Milwaukee Bucks       MIL  583ec8d4-fb46-11e1-82cb-f4ce4684ea4c   \n",
      "4       Memphis Grizzlies       MEM  583ed0ac-fb46-11e1-82cb-f4ce4684ea4c   \n",
      "5               Utah Jazz       UTA  583ecb8f-fb46-11e1-82cb-f4ce4684ea4c   \n",
      "6       Charlotte Hornets       CHA  583ecfa8-fb46-11e1-82cb-f4ce4684ea4c   \n",
      "7  Minnesota Timberwolves       MIN  583ece50-fb46-11e1-82cb-f4ce4684ea4c   \n",
      "8          Indiana Pacers       IND  583ec8d4-fb46-11e1-82cb-f4ce4684ea4c   \n",
      "9      Los Angeles Lakers       LAL  583ec5fd-fb46-11e1-82cb-f4ce4684ea4c   \n",
      "\n",
      "       srIdAway              nameAway aliasAway  \\\n",
      "0  sr:team:3409         Chicago Bulls       CHI   \n",
      "1  sr:team:3430     Charlotte Hornets       CHA   \n",
      "2  sr:team:3425  Los Angeles Clippers       LAC   \n",
      "3  sr:team:3431    Washington Wizards       WAS   \n",
      "4  sr:team:3413      Sacramento Kings       SAC   \n",
      "5  sr:team:3423         Atlanta Hawks       ATL   \n",
      "6  sr:team:3416          Phoenix Suns       PHX   \n",
      "7  sr:team:3434             Utah Jazz       UTA   \n",
      "8  sr:team:3431    Washington Wizards       WAS   \n",
      "9  sr:team:3409         Chicago Bulls       CHI   \n",
      "\n",
      "                               seasonId                createDt updatedDt  \n",
      "0  5027b6ac-731c-4622-8d69-d863ae7c626b 2022-08-27 15:49:00.720      None  \n",
      "1  feb33381-9dbf-45b6-82c1-3c50c9a2b5ce 2022-08-27 15:44:13.270      None  \n",
      "2  7dcb5184-ab33-49fe-bd18-c4ca2b1cfc08 2022-08-27 15:38:09.937      None  \n",
      "3  698590c2-3579-4d4e-a931-3d6dff427ee2 2022-08-27 15:33:34.293      None  \n",
      "4  03241c9e-731b-40d0-a36f-bcc9932d055b 2022-08-27 15:36:22.680      None  \n",
      "5  5027b6ac-731c-4622-8d69-d863ae7c626b 2022-08-27 15:48:26.633      None  \n",
      "6  16d6292c-25c6-4487-aa90-912c1e09170b 2022-08-27 15:46:14.807      None  \n",
      "7  47c9979e-5c3f-453d-ac75-734d17412e3f 2022-08-27 15:39:43.703      None  \n",
      "8  16d6292c-25c6-4487-aa90-912c1e09170b 2022-08-27 15:46:41.780      None  \n",
      "9  5ddc217a-a958-4616-9bdf-e081022c440b 2022-08-27 15:30:33.110      None  \n",
      "                               seasonId seasonType  seasonYear  \\\n",
      "0  03241c9e-731b-40d0-a36f-bcc9932d055b        REG        2016   \n",
      "1  16d6292c-25c6-4487-aa90-912c1e09170b        REG        2021   \n",
      "2  191e5d0e-ad55-43ef-995c-317ae9ea1213        REG        2019   \n",
      "3  1e20b802-0e4c-49f2-8045-f02a8105a71e        REG        2012   \n",
      "4  22ae1aa6-d28a-452f-b6ce-c120e03c6b7d        PST        2013   \n",
      "5  269501e2-f4fa-4328-ab65-4bafa7889f29        PRE        2017   \n",
      "6  31dbd74e-7b42-4b97-ae0f-a4e71a4221c4        PST        2018   \n",
      "7  47c9979e-5c3f-453d-ac75-734d17412e3f        REG        2018   \n",
      "8  5027b6ac-731c-4622-8d69-d863ae7c626b        REG        2022   \n",
      "9  529bed34-5a8d-46d4-9eef-114bd1340867        PST        2015   \n",
      "\n",
      "                 createDt updatedDt  \n",
      "0 2022-08-27 15:35:29.337      None  \n",
      "1 2022-08-27 15:45:29.320      None  \n",
      "2 2022-08-27 15:41:25.947      None  \n",
      "3 2022-08-27 15:29:06.253      None  \n",
      "4 2022-08-27 15:49:34.753      None  \n",
      "5 2022-08-27 15:37:17.050      None  \n",
      "6 2022-08-27 15:39:23.943      None  \n",
      "7 2022-08-27 15:39:34.297      None  \n",
      "8 2022-08-27 15:47:20.983      None  \n",
      "9 2022-08-27 15:50:13.280      None  \n"
     ]
    }
   ],
   "source": [
    "# Read in our newly loaded data\n",
    "games = pd.read_sql(\"SELECT TOP 10 * FROM scheduleBLOG\", cnxn)\n",
    "seasons = pd.read_sql(\"SELECT TOP 10 * FROM seasonBLOG\", cnxn)\n",
    "\n",
    "# Print a few games out\n",
    "print(games)\n",
    "# And a few seasons\n",
    "print(seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks to have worked. Eventually, a custom primary key that sorts chronologically would be nice. But, we have a good prototype to work with. We can now download some game/season statistic data and start having some real fun in the coming weeks and months. At least it will give me something to do this NBA season while I nervously wait for Chet Holmgren to [heal from his severe foot injury](https://theathletic.com/3535418/2022/08/24/thunder-chet-holmgren-out-for-season-lisfranc-injury/). Feel better soon, Chet!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52d5c7957a8de22acba2095602af51ba4a5f9cf4afefb365b970c2f72199ae60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
